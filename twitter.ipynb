{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TeeHunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importamos librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import nltk \n",
    "import re\n",
    "import string\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk import word_tokenize, pos_tag, pos_tag_sents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerias de nltk\n",
    "\n",
    "#### nltk.download('punkt')\n",
    "#### nltk.download('stopwords')\n",
    "#### nltk.download('tagsets')\n",
    "#### nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtener datos de la API "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar valor del Token en la aplicación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Cargar valores del archivo .env en las variables de entorno\n",
    "load_dotenv()\n",
    "# Cargar valor del token a variable\n",
    "bearer_token = os.environ.get(\"BEARER_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### definimos consulta a la API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.twitter.com/2/tweets/search/recent\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### definimos parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'query': '#venom lang:en -is:retweet',\n",
    "    'tweet.fields':'created_at',\n",
    "    'max_results':100\n",
    "}\n",
    "total_page = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### definimos cabecera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {bearer_token}\",\n",
    "    \"User-Agent\":\"TweeHunch\"\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener tweets recursivamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url, headers=headers, params=params)\n",
    "print(response)\n",
    "\n",
    "# Generar excepción si la respuesta no es exitosa\n",
    "if response.status_code != 200:\n",
    "    raise Exception(response.status_code, response.text)\n",
    "print(dict(response.json())['meta'])\n",
    "\n",
    "def get_data(url,params,total_page):\n",
    "    results = []\n",
    "    count = 0\n",
    "    while count < total_page:\n",
    "        count += 1\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        # Generar excepción si la respuesta no es exitosa\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(response.status_code, response.text)\n",
    "        data = response.json()['data']\n",
    "        meta_data = dict(response.json())['meta']\n",
    "        results.append(pd.json_normalize(data))\n",
    "        if 'next_token' not in meta_data:\n",
    "            break\n",
    "        else:\n",
    "            token = meta_data['next_token']\n",
    "            print(token)\n",
    "            params = {\n",
    "                'query': '#venom lang:en -is:retweet',\n",
    "                'tweet.fields':'created_at',\n",
    "                'next_token':token,\n",
    "                'max_results':100\n",
    "            }\n",
    "    return pd.concat(results)\n",
    "\n",
    "df = get_data(url,params, total_page)\n",
    "#df.drop(columns=['withheld.copyright','withheld.country_codes'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrar columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text']]\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardas tweets en csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizamos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_REGEX = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "MENTIONS_REGEX = r\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)\"\n",
    "HASHTAG_REGEX = r\"#\"\n",
    "\n",
    "df[\"text\"].replace(URL_REGEX,'',regex=True, inplace = True)\n",
    "df[\"text\"].replace(MENTIONS_REGEX,'',regex=True, inplace = True)\n",
    "df[\"text\"].replace(HASHTAG_REGEX,'',regex=True, inplace = True)\n",
    "df[\"text\"].replace(r\"[^A-Za-z0-9 | \\n]+\",' ',regex=True, inplace = True)\n",
    "df[\"text\"].replace(r\"\\t\",' ',regex=True, inplace = True)\n",
    "df[\"text\"].replace('[{}]'.format(string.punctuation),' ',regex=True, inplace = True)\n",
    "\n",
    "df[\"text\"] = df[\"text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizar\n",
    "\n",
    "tt = TweetTokenizer()\n",
    "\n",
    "tokenized_text = df['text'].apply(tt.tokenize)\n",
    "df[\"tokenized_text\"] = tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardas tweets tokenizados en csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tweets_tokenizados.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frecuencia de tweets tokenizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_list = df.explode('tokenized_text')\n",
    "# Obtener frecuencia de cada término\n",
    "fdist = FreqDist(tokenized_list['tokenized_text'])\n",
    "# Convertir a dataframe\n",
    "df_fdist = pd.DataFrame.from_dict(fdist, orient='index')\n",
    "df_fdist.columns = ['Frequency']\n",
    "df_fdist.index.name = 'Term'\n",
    "df_fdist.sort_values(by=['Frequency'], inplace=True, ascending=False)\n",
    "pd.set_option('display.max_rows', None)\n",
    "#df_fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nube de palabras de tweets tokenizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar nube de palabras\n",
    "wordcloud = WordCloud(max_words=1000, background_color=\"white\", collocations = False,min_font_size = 6).generate(df['tokenized_text'].to_string())\n",
    "\n",
    "# Mostrar gráfico\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.rcParams['figure.figsize'] = [100, 100]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "no_stopwords_data = []\n",
    "# Crear lista sin stopwords\n",
    "for x in tokenized_text:\n",
    "    for word in x:\n",
    "        if word.lower() not in stopwords:\n",
    "            no_stopwords_data.append(word)\n",
    "#no_stopwords_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertirmos la lista de stopwords a dataframe para tratar los datos para la frecuencia\n",
    "\n",
    "df_no_stopwords = pd.DataFrame(no_stopwords_data)\n",
    "#df_no_stopwords[df[\"no_stopwords\"].str.contains(\"the\")]\n",
    "df_no_stopwords.rename(columns={0: 'no_stopwords'}, inplace=True)\n",
    "#df_no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista de abecedario para filtrar las letras sueltas\n",
    "lista_abc = ['a','b','c','d','e','f','g','h','i','j','k','n','m','l','o','p','r','s','t','u','v','w','x','y','z']\n",
    "\n",
    "#filtrado de numeros, campos vacios y signos ? \n",
    "df_no_stopwords[\"no_stopwords\"].replace(r\"[ \\d | \\s | ?.* ]+\",'',regex=True, inplace = True)\n",
    "faltantes_index = df_no_stopwords[df_no_stopwords['no_stopwords'] == ''].index\n",
    "df_no_stopwords = df_no_stopwords.drop(faltantes_index, axis=0)\n",
    "\n",
    "#filtrado de las letras sueltas\n",
    "for i in range(len(lista_abc)):\n",
    "\tfaltantes_index = df_no_stopwords[df_no_stopwords['no_stopwords'] == lista_abc[i]].index\n",
    "\tdf_no_stopwords = df_no_stopwords.drop(faltantes_index, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_list = df_no_stopwords.explode('no_stopwords')\n",
    "# Obtener frecuencia de cada término\n",
    "fdist = FreqDist(tokenized_list['no_stopwords'])\n",
    "# Convertir a dataframe\n",
    "df_fdist = pd.DataFrame.from_dict(fdist, orient='index')\n",
    "df_fdist.columns = ['Frequency']\n",
    "df_fdist.index.name = 'Term'\n",
    "df_fdist.sort_values(by=['Frequency'], inplace=True, ascending=False)\n",
    "pd.set_option('display.max_rows', None)\n",
    "#df_fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "################################################################\n",
    "\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize = (50, 50))\n",
    "x = df_fdist[\"Term\"].tolist()\n",
    "y = df_fdist[\"Frequency\"].tolist()\n",
    "\n",
    "plt.show()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tweets tokenizados sin stop words a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tweets_without_stopwords.csv')\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nube de palabras de tweets tokenizados sin stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar nube de palabras\n",
    "data = \" \".join(map(str,no_stopwords_data))\n",
    "wordcloud = WordCloud(max_words=1000, background_color=\"white\", collocations = False,min_font_size = 6).generate(data)\n",
    "\n",
    "# Mostrar gráfico\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.rcParams['figure.figsize'] = [150, 150]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lematizacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicamos etiquetado POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_filter = df_no_stopwords[\"no_stopwords\"].tolist()\n",
    "\n",
    "#print(list_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener frecuencia de list_filter\n",
    "fdist = FreqDist(list_filter)\n",
    "# Convertir a dataframe\n",
    "df_fdist_list_filter = pd.DataFrame.from_dict(fdist, orient='index')\n",
    "df_fdist_list_filter.columns = ['Frequency']\n",
    "df_fdist_list_filter.index.name = 'Term'\n",
    "df_fdist_list_filter.sort_values(by=['Frequency'], inplace=True, ascending=False)\n",
    "\n",
    "#df_fdist_list_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar nube de palabras\n",
    "data = \" \".join(map(str,list_filter))\n",
    "wordcloud = WordCloud(max_words=1000, background_color=\"white\", collocations = False,min_font_size = 6).generate(data)\n",
    "\n",
    "# Mostrar gráfico\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.rcParams['figure.figsize'] = [150, 150]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etiquetar texto con pos_tag\n",
    "data_pos = nltk.pos_tag(list_filter)\n",
    "#data_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos patron regex\n",
    "p_adj = re.compile(r\"(^JJ)\")\n",
    "p_noun = re.compile(r\"(^NN)\")\n",
    "p_verb = re.compile(r\"(^VB)\")\n",
    "p_adverb = re.compile(r\"(^RB)\")\n",
    "\n",
    "adjectives, nouns, verbs, adverbs, others = [], [], [], [], []\n",
    "for k,v in data_pos:\n",
    "    if re.fullmatch(p_adj, v):\n",
    "        if k == \"venom\" or k == \"carnage\" or k == \"cr\":\n",
    "            pass\n",
    "        else:\n",
    "            adjectives.append(k)\n",
    "\n",
    "    elif re.fullmatch(p_noun, v):\n",
    "        if k == \"cr\":\n",
    "            pass\n",
    "        else:\n",
    "            nouns.append(k)\n",
    "\n",
    "    elif re.fullmatch(p_verb, v):\n",
    "        if k == \"venom\" or k == \"carnage\" or k == \"cr\":\n",
    "            pass\n",
    "        else:\n",
    "            verbs.append(k)\n",
    "\n",
    "    elif re.fullmatch(p_adverb, v):\n",
    "        if k == \"venom\" or k == \"carnage\" or k == \"cr\":\n",
    "            pass\n",
    "        else:\n",
    "            adverbs.append(k)\n",
    "            \n",
    "    else:\n",
    "        if k == \"venom\" or k == \"carnage\" or k == \"cr\":\n",
    "            pass\n",
    "        else:\n",
    "            others.append(k)\n",
    "\n",
    "#print(f'adjetivos: {adjectives} \\n\\n\\n\\n\\n sustantivos: {nouns} \\n\\n\\n\\n\\n\\n verbos: {verbs} \\n\\n\\n\\n\\n\\n adverbios:{adverbs} \\n\\n\\n\\n\\n\\n otros: {others}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener frecuencia de adjetivos\n",
    "fdist = FreqDist(adjectives)\n",
    "# Convertir a dataframe\n",
    "df_fdist_adj = pd.DataFrame.from_dict(fdist, orient='index')\n",
    "df_fdist_adj.columns = ['Frequency']\n",
    "df_fdist_adj.index.name = 'Term'\n",
    "df_fdist_adj.sort_values(by=['Frequency'], inplace=True, ascending=False)\n",
    "\n",
    "#df_fdist_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar nube de palabras\n",
    "data = \" \".join(map(str,adjectives))\n",
    "wordcloud = WordCloud(max_words=1000, background_color=\"white\", collocations = False,min_font_size = 6).generate(data)\n",
    "\n",
    "# Mostrar gráfico\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.rcParams['figure.figsize'] = [150, 150]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener frecuencia de sustantivos\n",
    "fdist = FreqDist(nouns)\n",
    "# Convertir a dataframe\n",
    "df_fdist_noun = pd.DataFrame.from_dict(fdist, orient='index')\n",
    "df_fdist_noun.columns = ['Frequency']\n",
    "df_fdist_noun.index.name = 'Term'\n",
    "df_fdist_noun.sort_values(by=['Frequency'], inplace=True, ascending=False)\n",
    "\n",
    "#df_fdist_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar nube de palabras\n",
    "data = \" \".join(map(str,nouns))\n",
    "wordcloud = WordCloud(max_words=1000, background_color=\"white\", collocations = False,min_font_size = 6).generate(data)\n",
    "\n",
    "# Mostrar gráfico\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.rcParams['figure.figsize'] = [150, 150]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener frecuencia de verbos\n",
    "fdist = FreqDist(verbs)\n",
    "# Convertir a dataframe\n",
    "df_fdist_verbs = pd.DataFrame.from_dict(fdist, orient='index')\n",
    "df_fdist_verbs.columns = ['Frequency']\n",
    "df_fdist_verbs.index.name = 'Term'\n",
    "df_fdist_verbs.sort_values(by=['Frequency'], inplace=True, ascending=False)\n",
    "\n",
    "#df_fdist_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar nube de palabras\n",
    "data = \" \".join(map(str,verbs))\n",
    "wordcloud = WordCloud(max_words=1000, background_color=\"white\", collocations = False,min_font_size = 6).generate(data)\n",
    "\n",
    "# Mostrar gráfico\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.rcParams['figure.figsize'] = [150, 150]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener frecuencia de adverbios\n",
    "fdist = FreqDist(adverbs)\n",
    "# Convertir a dataframe\n",
    "df_fdist_adv = pd.DataFrame.from_dict(fdist, orient='index')\n",
    "df_fdist_adv.columns = ['Frequency']\n",
    "df_fdist_adv.index.name = 'Term'\n",
    "df_fdist_adv.sort_values(by=['Frequency'], inplace=True, ascending=False)\n",
    "\n",
    "#df_fdist_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar nube de palabras\n",
    "data = \" \".join(map(str,adverbs))\n",
    "wordcloud = WordCloud(max_words=1000, background_color=\"white\", collocations = False,min_font_size = 6).generate(data)\n",
    "\n",
    "# Mostrar gráfico\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.rcParams['figure.figsize'] = [150, 150]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener frecuencia de adverbios\n",
    "fdist = FreqDist(others)\n",
    "# Convertir a dataframe\n",
    "df_fdist_other = pd.DataFrame.from_dict(fdist, orient='index')\n",
    "df_fdist_other.columns = ['Frequency']\n",
    "df_fdist_other.index.name = 'Term'\n",
    "df_fdist_other.sort_values(by=['Frequency'], inplace=True, ascending=False)\n",
    "\n",
    "#df_fdist_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar nube de palabras\n",
    "data = \" \".join(map(str,others))\n",
    "wordcloud = WordCloud(max_words=1000, background_color=\"white\", collocations = False, min_font_size = 6).generate(data)\n",
    "\n",
    "# Mostrar gráfico\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.rcParams['figure.figsize'] = [150, 150]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de polaridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSentiment =  pd.read_csv(\"tweets_without_stopwords.csv\")\n",
    "#dfSentiment.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "dfSentiment.drop(columns=['tokenized_text', 'Unnamed: 0'],inplace=True)\n",
    "#dfSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_REGEX = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "MENTIONS_REGEX = r\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)\" #(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)\n",
    "HASHTAG_REGEX = r\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))#([A-Za-z]+[A-Za-z0-9-_]+)\"\n",
    "\n",
    "dfSentiment[\"text\"].replace(URL_REGEX,'',regex=True, inplace = True)\n",
    "dfSentiment[\"text\"].replace(MENTIONS_REGEX,'',regex=True, inplace = True)\n",
    "dfSentiment[\"text\"].replace(HASHTAG_REGEX,'',regex=True, inplace = True)\n",
    "dfSentiment[\"text\"].replace(r\"[^A-Za-z0-9 | \\n]+\",' ',regex=True, inplace = True)\n",
    "dfSentiment[\"text\"].replace(r\"[\\t | \\n]\",' ',regex=True, inplace = True)\n",
    "dfSentiment[\"text\"].replace('[{}]'.format(string.punctuation),' ',regex=True, inplace = True)\n",
    "\n",
    "dfSentiment[\"text\"] = dfSentiment[\"text\"].str.lower()\n",
    "#dfSentiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar Analizador\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "dfSentiment[\"negative\"] = \"\"\n",
    "dfSentiment[\"neutral\"] = \"\"\n",
    "dfSentiment[\"positive\"] = \"\"\n",
    "dfSentiment[\"result\"] = \"\"\n",
    "count_negative = 0\n",
    "count_neutral = 0\n",
    "count_positive = 0\n",
    "\n",
    "for index, row in dfSentiment.iterrows():\n",
    "    #Analizar cada review\n",
    "    analisis = sentiment_analyzer.polarity_scores(row['text'])\n",
    "    row[\"negative\"] = analisis[\"neg\"]\n",
    "    row[\"neutral\"] = analisis[\"neu\"]\n",
    "    row[\"positive\"] = analisis[\"pos\"]\n",
    "    # Evaluar que valores se considerarán positivo o negativo\n",
    "    if analisis['compound'] >= 0.45 :\n",
    "        count_positive += 1\n",
    "        row[\"result\"] = \"Positive\"\n",
    "    elif analisis['compound'] <= -0.24 :\n",
    "        count_negative += 1\n",
    "        row[\"result\"] = \"Negative\"\n",
    "    else :\n",
    "        count_neutral += 1\n",
    "        row[\"result\"] = \"Neutral\"\n",
    "total = count_negative+count_neutral+count_positive\n",
    "print(f'positivo: {count_positive} negativo: {count_negative} neutral: {count_neutral} total: {total}')\n",
    "print(f'positivo: {count_positive/total} negativo: {count_negative/total} neutral: {count_neutral/total}')\n",
    "#dfSentiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCloud de positivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "df_aux = dfSentiment[dfSentiment['result'] == 'Positive']\n",
    "\n",
    "# Tokenizar\n",
    "tt = TweetTokenizer()\n",
    "\n",
    "tokenized_text = df_aux['text'].apply(tt.tokenize)\n",
    "df_aux[\"tokenized_text\"] = tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "no_stopwords_data = []\n",
    "# Crear lista sin stopwords\n",
    "for x in tokenized_text:\n",
    "    for word in x:\n",
    "        if word.lower() not in stopwords:\n",
    "            no_stopwords_data.append(word)\n",
    "#print(no_stopwords_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \" \".join(map(str,no_stopwords_data))\n",
    "# Generar nube de palabras\n",
    "wordcloud = WordCloud(max_words=1000, background_color=\"white\", collocations = False, min_font_size = 20).generate(data)\n",
    "\n",
    "# Mostrar gráfico\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.rcParams['figure.figsize'] = [150, 150]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCloud de negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux = dfSentiment[dfSentiment['result'] == 'Negative']\n",
    "\n",
    "# Tokenizar\n",
    "tt = TweetTokenizer()\n",
    "\n",
    "tokenized_text = df_aux['text'].apply(tt.tokenize)\n",
    "df_aux[\"tokenized_text\"] = tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "no_stopwords_data = []\n",
    "# Crear lista sin stopwords\n",
    "for x in tokenized_text:\n",
    "    for word in x:\n",
    "        if word.lower() not in stopwords:\n",
    "            no_stopwords_data.append(word)\n",
    "#print(no_stopwords_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \" \".join(map(str,no_stopwords_data))\n",
    "# Generar nube de palabras\n",
    "wordcloud = WordCloud(max_words=1000, background_color=\"white\", collocations = False, min_font_size = 20).generate(data)\n",
    "\n",
    "# Mostrar gráfico\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.rcParams['figure.figsize'] = [150, 150]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweet mas negativo\n",
    "pd.set_option('display.max_columns', None)  \n",
    "print(df_aux)\n",
    "for i in df_aux['text']:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "41805f2a7e02b28922c6689937d05cf098c6ed4a0aa734823cacf02002620d85"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
